% Template v2.0: 11/13/2018.   The previous template is also acceptable. 
% Template for white paper submissions for the 
% LSST Call for Observing Strategies for DeepDrilling and Minisurveys 
% 
% The call for white papers can be found at https://github.com/lsst-pst/survey_strategy/blob/master/latex/WPcall2018.pdf
% The deadline for submissions is November 30, 2018
% To submit white papers, please email the compiled PDF to lsst-survey-strategy@lists.lsst.org   
%  OR submit a pull request to this github repository (github.com/lsst-pst/survey_strategy_wp) with your white paper in a clearly named subdirectory.
% For help with white papers or the submission process, please post at http://community.lsst.org/c/sci/survey-strategy


\documentclass[12pt, letterpaper]{article}
\usepackage[top=1in, bottom=1.5in, left=1in, right=1in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{booktabs}
\usepackage{natbib}
\usepackage[colorlinks=true,linkcolor=blue, citecolor=blue, linkcolor=red]{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\definecolor{green}{HTML}{228B22}

\title{Unveiling the Rich and Diverse Universe of Subsecond Astrophysics through LSST Star Trails}
\author{David Thomas, Steven Kahn, Federica Bianco, {\color{green}More Coming Soon}}
\date{May 2018}

\begin{document}

\maketitle

\begin{abstract}
We present a unique method that allows the LSST to scan the sky for subsecond stellar variability. The method has operational and image processing components. The operational component is to take star trail images, which facilitate sub-exposure photometry. The image processing component is to use deep learning to sift for transient events on timescales down to 10 ms. We advocate for coupling this capability with the LSST's unrivaled 319.5 m$^2$deg$^2$ etendue to produce the first optical survey of the universe on these timescales. We explain how this data will advance both planned lines of investigation and enable new research in the areas of stellar flares, cataclysmic variables, active galactic nuclei, Kuiper Belt objects, gamma-ray bursts, and fast radio bursts.
\end{abstract}

\section{White Paper Information}
The corresponding author is David Thomas (dthomas5@stanford.edu).

\begin{enumerate} 
\item {\bf Science Category:} We introduce a mechanism that enables the LSST to provide new data with much higher time resolution that not only enhances existing investigations, but allows LSST to contribute to new science use cases that generally lie within the \textit{Exploring the Changing Sky} and \textit{Milky Way Structure and Formation} categories.

\item {\bf Survey Type Category:} We propose inserting occasional star trail images into the main \textit{Wide-Fast-Deep} survey.

\item {\bf Observing Strategy Category:}
While different fields are conducive to different aspects of our method - for example, searching open clusters for flare stars - it is largely agnostic of where the telescope is pointed. Furthermore, our proposal can be trivially interleaved with the main LSST survey.

\end{enumerate}  


\clearpage

\section{Scientific Motivation}

\begin{footnotesize}
{\it Describe the scientific justification for this white paper in the context
of your field, as well as the importance to the general program of astronomy, 
including the relevance over the next decade. 
Describe other relevant data, and justify why LSST is the best facility for these observations.
(Limit: 2 pages + 1 page for figures.)}
\end{footnotesize}
\\

A wide range of astrophysical phenomena ranging from local Kuiper Belt object occultations to cosmic gamma-ray bursts manifest on subsecond time scales. Conventional optical telescopes rely on charge-coupled devices (CCDs) which typically take around ten seconds to read out. This readout time limits the time resolution they can achieve and precludes them from participating in high time resolution investigations. Furthermore, the special instruments that can image optical bands at high speeds have fields of view that are typically a few arcminutes, or less than 1/1000th of the LSST field of view. We present a mechanism that allows the LSST to explore the subsecond universe and describe how this unique data will (i) enhance planned LSST investigations of active galactic nuclei (AGN), stellar flares, and exoplanets and (ii) enable new LSST science with Kuiper Belt objects, fast radio bursts (FRBs), gamma-ray bursts (GRBs), and cataclysmic variables.

This proposal relies on a key insight originally from \cite{1986PASP...98..802H} and further developed in \cite{mine}: star trail images are a conduit to achieving subsecond photometry of stellar sources. In star trail images, the tracking is turned off so the telescope rotates with the Earth during the exposure. Stellar sources are stretched into coherent linear trails, which show how the flux of the sources changes throughout the exposure. Figure \ref{fig:trail} shows a simulated LSST star trail image with a one second exposure time. This choice of exposure time is elaborated on in Section \ref{sec:technical}. We then train a deep neural network to scan these large, unorthodox images and detect variability.

The input to the network is a 80x80 pixel crop of an LSST star trail image, the output is a binary classification which determines whether the sample is worth following up. As in many deep learning applications, high quality data and training feedback are essential. We use a suite of LSST simulation tools to produce realistic images. We sample visits from the \textit{minion\_1016} OpSim observing run, then we use CatSim to procure catalogs for each visit, then we use PhoSim to produce high fidelity simulated images of the catalogs \citep{2014SPIE.9150E..15D,2014SPIE.9150E..14C,2015ApJS..218...14P}. We add a new interface into the PhoSim code to simulate \textit{bursts} - a tophat change in flux added to an otherwise flat and static light curve of a source - parameterized by the magnitude change and duration. We train the network over 5 epochs of 80,000 sample 80x80 pixel crops, half of which contain a burst. We train the nextwork to both predict whether the burst exists and to predict the exact photons resulting from the change in flux. Figure \ref{fig:pipeline} highlights this process. 

We assess the performance of our technique on visits and corresponding images that the network was not trained on. Figure \ref{fig:limit} shows the results. These results are competitive with the state of the art \citep{2016SPIE.9908E..0YD}. 
\\

{\color{green}
\noindent AGN/Blazars\\
Stellar Flares\\
Exoplanets\\
Gamma-Ray Bursts\\
Kuiper Belt Objects\\
Fast Radio Bursts\\
Cataclysmic Variables\\
}

% Immense potential. We look forward to further developing our technique, validating it on real data from DECam, and refining it for specific science cases. 
Conclusion.

\
\begin{figure*}[htb]
\center
\includegraphics[width=1.00\columnwidth]{star_trail.pdf}
\caption{\textit{Left:} a star trail image corresponding to a 1 second exposure on a single LSST CCD in the `r' filter. \textit{Middle:} zoom-in of a single star trail that is in the green box region in the full image. \textit{Right:} zoom-in of the extra flux due to the burst.}
\label{fig:trail}
% http://localhost:8888/notebooks/Code/Astronomy/exp7/MakeWhitePaperGraphics_20181120.ipynb
\end{figure*}

\begin{figure*}[htb]
\center
\includegraphics{bbox.jpeg}
\caption{Image processing pipeline.}
\label{fig:pipeline}
% http://localhost:8888/notebooks/Code/Astronomy/exp4/notebooks/MakeVariabilityGraphic_20180329-Copy1ForICMETalk.ipynb
\end{figure*}

\begin{figure*}[htb]
\center
\includegraphics{bbox.jpeg}
\caption{Detection accuracy and performance limits.}
\label{fig:limit}
% http://localhost:8888/notebooks/Code/Astronomy/exp4/notebooks/MakeVariabilityGraphic_20180329-Copy1ForICMETalk.ipynb
\end{figure*}

\vspace{.6in}

% \section{Technical Description}
% \label{sec:technical}
% \begin{footnotesize}
% {\it Describe your survey strategy modifications or proposed observations. Please comment on each observing constraint
% below, including the technical motivation behind any constraints. Where relevant, indicate
% if the constraint applies to all requested observations or a specific subset. Please note which 
% constraints are not relevant or important for your science goals.}
% \end{footnotesize}

% \subsection{Overview}

% The crux of our proposal is taking star trail exposures where the tracking is turned off. The slow rotation of the Earth during the image with respect to the sky we image, produces star trail images. While these images have been employed for site-testing, during the early days of CCDs, and for popular science images that can be found by a Google search - they have not yet been employed at the scale we are proposing. We have a gradual process to validate that this method can deliver on the science we aspire to.

% We have demonstrated that in a wide variety of high fidelity simulated images our method can effectively detect variability. The next step is to test it on real data. We have proposed to do this and will be taking a half-nights worth of data on DECam in the second half of 2019. This will help us adress many problems.

% The next stage of testing will take place during comissioning. Through private communication with Steven Kahn and Chuck Claver, we know that there will be ample opportunity to take these images. This data will allow us to further iterate. 

% While perhaps this has been done to make popular star trail images, it has never been done at the scale we are proposing. 

% There are three modes of operation:

% Strong Signal-to-Noise:

% High duty cycle:

% Strategic Rotations:


% \subsection{Constraints}

% \begin{itemize}
% \item \textbf{Footprint.} We have not yet developed a footprint optimized for the explorations made possible by star trail images. We plan to continue evaluating specific use cases and aim to provide comprehensive footprint guideance by Summer 2019. We expect the following details to be relevant. The length $l$ of a star trail depends on the declination $\delta$, $l = 1071 \cdot \cos(\delta)$ pixels for the LSST. Star trail data complements existing surveys, including some in other regions of the electromagnetic spectrum. These considerations will likely provide weak constraints on the optimal footprint. 

% \item \textbf{Image Quality.} 

% \item \textbf{Sky Brightness.}

% \item \textbf{Total Number of Visits.}

% \item \textbf{Number of Visits Within Night.}

% \item \textbf{Distribution of Visits Over Time.}

% \item \textbf{Filter Choice.}

% \item \textbf{Exposure Constraints.}

% \end{itemize}

% \begin{table}[ht]
%     \centering
%     \begin{tabular}{l|l|l|l}
%         \toprule
%         Properties & Importance \hspace{.3in} \\
%         \midrule
%         Image quality & 2\\
%         Sky brightness & 1\\
%         Individual image depth & 3\\
%         Co-added image depth & 3\\
%         Number of exposures in a visit & 3\\
%         Number of visits (in a night) & 3\\ 
%         Total number of visits & 1\\
%         Time between visits (in a night) & 3\\
%         Time between visits (between nights)  & 3\\
%         Long-term gaps between visits & 3\\
%         \bottomrule
%     \end{tabular}
%     \caption{{\bf Constraint Rankings:} Summary of the relative importance of various survey strategy constraints. Please rank the importance of each of these considerations, from 1=very important, 2=somewhat important, 3=not important. If a given constraint depends on other parameters in the table, but these other parameters are not important in themselves, please only mark the final constraint as important. For example, individual image depth depends on image quality, sky brightness, and number of exposures in a visit; if your science depends on the individual image depth but not directly on the other parameters, individual image depth would be `1' and the other parameters could be marked as `3', giving us the most flexibility when determining the composition of a visit, for example.}
%         \label{tab:obs_constraints}
% \end{table}


% \vspace{.3in}

% \subsection{Footprint -- pointings, regions and/or constraints}
% \begin{footnotesize}{\it Describe the specific pointings or general region (RA/Dec, Galactic longitude/latitude or 
% Ecliptic longitude/latitude) for the observations. Please describe any additional requirements, especially if there
% are no specific constraints on the pointings (e.g. stellar density, galactic dust extinction).}
% \end{footnotesize}

% We have not yet constrainted this.

% \subsection{Image quality}
% \begin{footnotesize}{\it Constraints on the image quality (seeing).}\end{footnotesize}

% We are sensitive to widening of the trail in the transverse direction, we are not too sensitive.

% \subsection{Individual image depth and/or sky brightness}
% \begin{footnotesize}{\it Constraints on the sky brightness in each image and/or individual image depth for point sources.
% Please differentiate between motivation for a desired sky brightness or individual image depth (as 
% calculated for point sources). Please provide sky brightness or image depth constraints per filter.}
% \end{footnotesize}

% Sky brightness is an issue and image depth is not a concern because we do not coadd.

% \subsection{Co-added image depth and/or total number of visits}
% \begin{footnotesize}{\it  Constraints on the total co-added depth and/or total number of visits.
% Please differentiate between motivations for a given co-added depth and total number of visits. 
% Please provide desired co-added depth and/or total number of visits per filter, if relevant.}
% \end{footnotesize}

% What we care about

% \subsection{Number of visits within a night}
% \begin{footnotesize}{\it Constraints on the number of exposures (or visits) in a night, especially if considering sequences of visits.  }
% \end{footnotesize}

% \subsection{Distribution of visits over time}
% \begin{footnotesize}{\it Constraints on the timing of visits --- within a night, between nights, between seasons or
% between years (which could be relevant for rolling cadence choices in the WideFastDeep. 
% Please describe optimum visit timing as well as acceptable limits on visit timing, and options in
% case of missed visits (due to weather, etc.). If this timing should include particular sequences
% of filters, please describe.}
% \end{footnotesize}

% \subsection{Filter choice}
% \begin{footnotesize}
% {\it Please describe any filter constraints not included above.}
% \end{footnotesize}

% \subsection{Exposure constraints}
% \begin{footnotesize}
% {\it Describe any constraints on the minimum or maximum exposure time per visit required (or alternatively, saturation limits).
% Please comment on any constraints on the number of exposures in a visit.}
% \end{footnotesize}

% \subsection{Other constraints}
% \begin{footnotesize}
% {\it Any other constraints.}
% \end{footnotesize}

% \subsection{Estimated time requirement}
% \begin{footnotesize}
% {\it Approximate total time requested for these observations, using the guidelines available at \url{https://github.com/lsst-pst/survey_strategy_wp}.}
% \end{footnotesize}

% \vspace{.3in}



% \subsection{Technical trades}
% \begin{footnotesize}
% {\it To aid in attempts to combine this proposed survey modification with others, please address the following questions:
% \begin{enumerate}
%     \item What is the effect of a trade-off between your requested survey footprint (area) and requested co-added depth or number of visits?
%     \item If not requesting a specific timing of visits, what is the effect of a trade-off between the uniformity of observations and the frequency of observations in time? e.g. a `rolling cadence' increases the frequency of visits during a short time period at the cost of fewer visits the rest of the time, making the overall sampling less uniform.
%     \item What is the effect of a trade-off on the exposure time and number of visits (e.g. increasing the individual image depth but decreasing the overall number of visits)?
%     \item What is the effect of a trade-off between uniformity in number of visits and co-added depth? Is there any benefit to real-time exposure time optimization to obtain nearly constant single-visit limiting depth?
%     \item Are there any other potential trade-offs to consider when attempting to balance this proposal with others which may have similar but slightly different requests?
% \end{enumerate}}
% \end{footnotesize}

% \section{Performance Evaluation}
% \begin{footnotesize}
% {\it Please describe how to evaluate the performance of a given survey in achieving your desired
% science goals, ideally as a heuristic tied directly to the observing strategy (e.g. number of visits obtained
% within a window of time with a specified set of filters) with a clear link to the resulting effect on science.
% More complex metrics which more directly evaluate science output (e.g. number of eclipsing binaries successfully
% identified as a result of a given survey) are also encouraged, preferably as a secondary metric.
% If possible, provide threshold values for these metrics at which point your proposed science would be unsuccessful 
% and where it reaches an ideal goal, or explain why this is not possible to quantify. While not necessary, 
% if you have already transformed this into a MAF metric, please add a link to the code (or a PR to 
% \href{https://github.com/lsst-nonproject/sims_maf_contrib}{sims\_maf\_contrib}) in addition to the text description. (Limit: 2 pages).}
% \end{footnotesize}

% \vspace{.6in}

% \section{Special Data Processing}
% \begin{footnotesize}
% {\it Describe any data processing requirements beyond the standard LSST Data Management pipelines and how these will be achieved. Simon Krughoff}
% \end{footnotesize}


\bibliographystyle{aasjournal}
\bibliography{references}
\end{document}